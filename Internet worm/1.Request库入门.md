# Python网络爬虫与信息提取
- 目标：掌握定向网络数据爬取和网页解析的基本能力
- 爬虫路径：
![avatar](E:/學習記錄/Python学习之路/images/爬虫路径.png)

## Request库入门
### Requests库的7个主要方法
![avatar](E:/學習記錄/Python学习之路/images/requests主要方法.png)

- `r = requests.get(url)`
    - `r`       ：返回一个包含服务器资源的`Response`对象
    - `get`     ：构造一个向服务器请求资源的`Request`对象

- `requests.get(url, params=None, **kwargs)`
    - `url`       ： 拟获取页面的url链接
    - `params`    ： `url`中的额外参数，字典或字节流格式，可选
    - `**kwargs`  ： 12个控制访问的参数

- `requests.head(url, **kwargs)`

- `requests.post(url, data=None, json=None, **kwargs)`
    + `data`    ： 字典、字节序列或文件，`Request`的内容
    + `json`    ： JSON格式的数据，`Request`的内容

- `requests.put(url, data=None, **kwargs)`

- `request.patch(url, data=None, **kwargs)`

- `requests.delete(url, **kwargs)`

- `requests.request(method, url, **kwargs)`
    + `method`  ：请求方式，对应`get/put/post`等7种
    + `**kwargs`    ：控制访问的参数，共13个，可选
```python
method:
r = requests.request('GET',url,**kwargs)
r = requests.request('HEAD',url,**kwargs)
r = requests.request('POST',url,**kwargs)
r = requests.request('PUT',url,**kwargs)
r = requests.request('PATCH',url,**kwargs)
r = requests.request('delete',url,**kwargs)
r = requests.request('OPTIONS',url,**kwargs)

**kwargs
params: 字典或字节序列，作为参数增加到url中
data: 字典、字节序列或文件对象，作为Request的内容
json: JSON格式的数据，作为Request的内容
header: 字典，HTTP定制头
cookies: 字典或CookiesJar，Request中的cookie
auth: 元组，支持HTTP认证功能
files: 字典类型，传输文件
timeout: 设定超时时间，秒为单位
proxies: 字典类型，设定访问代理服务器，可以增加登录认证
allow_redirects: True/False，默认为True，重定向开关
stream: True/False，默认为True，获取内容立即下载开关
verify: True/False，默认为True，认证SSL证书开关
cert: 本地SSL证书路径
```

### Requests库的2个重要对象
- Response对象
    - 包含爬虫返回的内容。包含服务器返回的所有信息，也包含请求的Request信息。
![avatar](E:/學習記錄/Python学习之路/images/Response对象的属性.png)

- `r.status_code`等于200时才可以继续后续操作，等于404或其他，会因为某些原因出错将产生异常。

- `r.encoding`：如果header中不存在charset，则认为编码为ISO-8859-1。`r.text`根据`r.encoding`显示网页内容
- `r.apparent_encoding`：根据网页内容分析出的编码方式，可以看作是`r.encoding`的备选

### 理解Requests库的异常
![avatar](E:/學習記錄/Python学习之路/images/Requests的异常.png)

### 理解Response的异常
`r.raise_for_status()`如果不是200，产生异常 `requests.HTTPError`

- `r.raise_for_status()`在方法内部判断`r.status_code`是否等于200，不需要增加额外的if语句，该语句便于利用`try-except`进行异常处理

### HTTP协议
HTTP，Hypertext Transfer Protocol，超文本传输协议

- HTTP是一个基于"请求与响应"模式的、无状态的应用层协议
- HTTP协议采用URL作为定位网络资源的标识，URL格式如下：`http://host[:post][path]`
    - host: 合法的Internet主机域名或IP地址
    - port: 端口号，缺省端口为80
    - path: 请求资源的路径
- HTTP URL的理解：URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源

### HTTP协议对资源的操作
![avatar](E:/學習記錄/Python学习之路/images/HTTP协议对资源的操作.png)

- 通过URL和命令管理资源，操作独立无状态，网络通道及服务器成为了黑盒子

### 理解PATCH和PUT的区别
假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段
需求：用户修改了UserName，其他不变

- 采用PATCH，仅向URL提交UserName的局部更新请求
- 采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除
- PATCH的最主要好处：节省网络带宽

### 爬取网页的通用代码框架
```python
try:
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    return r.text
except:
    return "产生异常"
```
